{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hw 3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ava1CmgoZrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeh-DbYMrF-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmR3baCGrSu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_PATH = 'data.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrXIFhsSrJ87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "download = drive.CreateFile({'id': '1Q7vQ2SgsSucrMn-zNHS_D3RKJ3GjSZ5m'})\n",
        "download.GetContentFile(DATA_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDicknGUrdCz",
        "colab_type": "code",
        "outputId": "51a695f5-5f38-4fa1-f956-c3ac74af00b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pickle\n",
        "import torch\n",
        "\n",
        "\n",
        "with open(DATA_PATH, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "print(data.keys())\n",
        "data['train'] = torch.FloatTensor(data['train']).permute(0, 3, 1, 2)\n",
        "data['test'] = torch.FloatTensor(data['test']).permute(0, 3, 1, 2)"
      ],
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['train', 'test'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCHi2Id7ruUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def show(images):\n",
        "    images = images.permute(0, 2, 3, 1)\n",
        "    fig=plt.figure(figsize=(8, 8))\n",
        "    columns = images.shape[0]\n",
        "    rows = 1\n",
        "    for i, img in enumerate(images):\n",
        "        fig.add_subplot(rows, columns, i + 1)\n",
        "        plt.imshow(img / 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpvskQE6r-vz",
        "colab_type": "code",
        "outputId": "5c99be8c-b608-41f3-88c3-cf771701af4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "show(data['train'][:5])"
      ],
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAABvCAYAAAA0RRMsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 576x576 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVLT5Bu9sCvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class Resnet(nn.Module):\n",
        "    def __init__(self, ch_in, n_filters=256, n_blocks=8):\n",
        "        super(Resnet,self).__init__()\n",
        "        ch_out = ch_in * 2\n",
        "        self.n_blocks = n_blocks\n",
        "       # self.norm1 =  nn.BatchNorm2d(n_filters)\n",
        "        self.conv1 = nn.Conv2d(ch_in, n_filters, kernel_size=(3, 3), stride=(1, 1), padding=2)\n",
        "        get_ht_model = lambda: nn.Sequential(\n",
        "             nn.BatchNorm2d(n_filters),\n",
        "            nn.Conv2d(n_filters, n_filters, kernel_size=(1, 1), stride=(1, 1), padding=0),\n",
        "            nn.ReLU(),\n",
        "             nn.BatchNorm2d(n_filters),\n",
        "            nn.Conv2d(n_filters, n_filters, kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
        "        )\n",
        "        get_h_model = lambda: nn.Sequential(\n",
        "            nn.BatchNorm2d(n_filters),\n",
        "            nn.Conv2d(n_filters, n_filters, kernel_size=(1,1), stride=(1, 1), padding=0),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self._h_model = torch.nn.ModuleList([get_ht_model() for _ in range(n_blocks)])\n",
        "        self.h_model = torch.nn.ModuleList([get_h_model() for _ in range(n_blocks)])\n",
        "        #self.h_sum = torch.nn.ModuleList([nn.ReLU() for _ in range(n_blocks)])\n",
        "     #   self.norm2 =  nn.BatchNorm2d(n_filters)\n",
        "        self.conv2 = nn.Conv2d(n_filters, ch_out, kernel_size=(3, 3), stride=(1, 1))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.conv1(x)\n",
        "        for i in range(self.n_blocks):\n",
        "            _h = self._h_model[i](h)\n",
        "            h = self.h_model[i](_h)\n",
        "            h = torch.cat((h, _h), dim=0)\n",
        "        h = F.relu(h)\n",
        "      #  h = self.norm2(h)\n",
        "        x = self.conv2(h)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNegiyISsUvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AffineCoupling(nn.Module):\n",
        "    def __init__(self, ch_in, sign):\n",
        "        super(AffineCoupling, self).__init__()\n",
        "        self.resnet = Resnet(ch_in)\n",
        "        self.sign = sign\n",
        "    \n",
        "    def forward(self, x, masks):\n",
        "        (x1, x2) = x\n",
        "        y1 = x1\n",
        "        log_s, t = torch.chunk(self.resnet(x1), 2, dim=1)\n",
        "        y2 = torch.exp(log_s) * (x2 + t) # TODO: y1 or x2?\n",
        "        log_det = log_s.view(x1.shape[0], -1).sum(dim=1) * self.sign\n",
        "        return ((y1, y2), log_det)\n",
        "\n",
        "    def reverse(self, y, mask):\n",
        "        (y1, y2) = y\n",
        "        x1 = y1\n",
        "        log_s, t = torch.chunk(self.resnet(x1), 2, dim=1)\n",
        "        x2 = y2 * torch.exp(-log_s) - t * (1 - mask[1])\n",
        "        return (x1, x2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA9l-ok2s3eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CelebA(nn.Module):\n",
        "  def __init__(self, ch_in):\n",
        "      super(CelebA, self).__init__()\n",
        "      self.prior = torch.distributions.Normal(torch.tensor(0.).to(device), torch.tensor(1.).to(device))\n",
        "      signs = [1, 1, 1, 1] # TODO: maybe log_det_J after tuple flip must change signs?\n",
        "      self.couplings1 = nn.ModuleList([AffineCoupling(ch_in, signs[i]) for i in range(4)])\n",
        "      self.couplings2 = nn.ModuleList([AffineCoupling(ch_in * 4, signs[i]) for i in range(3)])\n",
        "      self.couplings3 = nn.ModuleList([AffineCoupling(ch_in * 4, signs[i]) for i in range(3)])\n",
        "      self.couplings4 = nn.ModuleList([AffineCoupling(ch_in * 16, signs[i]) for i in range(3)])\n",
        "      self.couplings5 = nn.ModuleList([AffineCoupling(ch_in * 16, signs[i]) for i in range(3)])\n",
        "\n",
        "  def build_mask(self, size, config=1.):\n",
        "      mask = np.arange(size).reshape(-1, 1) + np.arange(size)\n",
        "      mask = np.mod(config + mask, 2)\n",
        "      mask = mask.reshape(-1, 1, size, size)\n",
        "      return torch.tensor(mask.astype('float32'))\n",
        "\n",
        "  def flip_tuple(self, x, masks):\n",
        "      (x1, x2) = x\n",
        "      (mask1, mask2) = masks\n",
        "      return (x2, x1), (mask2, mask1)\n",
        "\n",
        "  def checkerboard_split(self, x):\n",
        "      mask = self.build_mask(x.shape[2], config=1.).to(device)\n",
        "      return (x * mask, x * (1 - mask)), (mask, 1 - mask) # TODO: is it correct split?\n",
        "\n",
        "  def inverse_checkerboard_split(self, x):\n",
        "      return x[0] + x[1] # TODO: is it correct split?\n",
        "\n",
        "  def squeeze(self, x):\n",
        "      x = x.reshape(-1, 4 * x.shape[1], x.shape[2] // 2, x.shape[3] // 2)\n",
        "      return x\n",
        "  \n",
        "  def unsqueeze(self, x):\n",
        "      #x = x.permute(0, 2, 3, 1)\n",
        "      #x = x.reshape(-1, x.shape[1] * 2, x.shape[2] * 2, x.shape[3] // 4)\n",
        "      #x = x.permute(0, 3, 1, 2)\n",
        "      x = x.reshape(-1, x.shape[1] // 4, x.shape[2] * 2, x.shape[3] * 2)\n",
        "      return x\n",
        "\n",
        "  \n",
        "  def channel_split(self, x):\n",
        "      mask = torch.zeros((1, x.shape[1], x.shape[2], x.shape[3])).to(device)\n",
        "      mask[:, :mask.shape[1] // 2, :, :] = 1\n",
        "      return (x * mask, x * (1 - mask)), (mask, 1 - mask)\n",
        "\n",
        "  def inverse_channel_split(self, x):\n",
        "      return x[0] + x[1]\n",
        "\n",
        "  def forward(self, x):\n",
        "      log_det = 0\n",
        "\n",
        "      x, masks = self.checkerboard_split(x) \n",
        "      for i in range(4):\n",
        "          x, log_det_temp = self.couplings1[i](x, masks)\n",
        "          log_det += log_det_temp\n",
        "          x, masks = self.flip_tuple(x, masks)\n",
        "      x = self.inverse_checkerboard_split(x)\n",
        "\n",
        "      x = self.squeeze(x)\n",
        "      \n",
        "      x, masks = self.channel_split(x)\n",
        "      for i in range(3):\n",
        "          x, log_det_temp = self.couplings2[i](x, masks)\n",
        "          log_det += log_det_temp\n",
        "          x, masks = self.flip_tuple(x, masks)\n",
        "      x = self.inverse_channel_split(x)\n",
        "      x, masks = self.checkerboard_split(x)\n",
        "      for i in range(3):\n",
        "          x, log_det_temp = self.couplings3[i](x, masks)\n",
        "          log_det += log_det_temp\n",
        "          x, masks = self.flip_tuple(x, masks)\n",
        "      x = self.inverse_checkerboard_split(x)\n",
        "      \n",
        "      x = self.squeeze(x)\n",
        "      \n",
        "      x, masks = self.channel_split(x)\n",
        "      for i in range(3):\n",
        "          x, log_det_temp = self.couplings4[i](x, masks)\n",
        "          log_det += log_det_temp\n",
        "          x, masks = self.flip_tuple(x, masks)\n",
        "      x = self.inverse_channel_split(x)\n",
        "\n",
        "      x, masks = self.checkerboard_split(x)\n",
        "      for i in range(3):\n",
        "          x, log_det_temp = self.couplings5[i](x, masks)\n",
        "          log_det += log_det_temp\n",
        "          x, masks = self.flip_tuple(x, masks)\n",
        "      x = self.inverse_checkerboard_split(x)\n",
        "\n",
        "      #x = self.unsqueeze(self.unsqueeze(x)) # TODO: Is it correct to do so?\n",
        "\n",
        "      return x, log_det\n",
        "    \n",
        "  def reverse(self, z):\n",
        "      x = z\n",
        "      #x = self.squeeze(self.squeeze(x))\n",
        "      \n",
        "      x, masks = self.checkerboard_split(x)\n",
        "      for i in range(3):\n",
        "          x, masks = self.flip_tuple(x, masks)\n",
        "          x = self.couplings5[i].reverse(x, masks)\n",
        "      x = self.inverse_checkerboard_split(x)\n",
        "\n",
        "      x, masks = self.channel_split(x)\n",
        "      for i in range(3):\n",
        "          x, masks = self.flip_tuple(x, masks)\n",
        "          x = self.couplings4[i].reverse(x, masks)\n",
        "      x = self.inverse_channel_split(x)\n",
        "\n",
        "      x = self.unsqueeze(x)\n",
        "\n",
        "      x, masks = self.checkerboard_split(x)\n",
        "      for i in range(3):\n",
        "          x, masks = self.flip_tuple(x, masks)\n",
        "          x = self.couplings3[i].reverse(x, masks)\n",
        "      x = self.inverse_checkerboard_split(x)\n",
        "\n",
        "      x, masks = self.channel_split(x)\n",
        "      for i in range(3):\n",
        "          x, masks = self.flip_tuple(x, masks)\n",
        "          x = self.couplings2[i].reverse(x, masks)\n",
        "      x = self.inverse_channel_split(x)\n",
        "      \n",
        "      x = self.unsqueeze(x)\n",
        "      \n",
        "      x, masks = self.checkerboard_split(x) \n",
        "      for i in range(4):\n",
        "          x, masks = self.flip_tuple(x, masks)\n",
        "          x = self.couplings1[i].reverse(x, masks)\n",
        "      x = self.inverse_checkerboard_split(x)\n",
        "\n",
        "      return x\n",
        "    \n",
        "  def log_prob(self, x):\n",
        "      z, log_det_J = self(x)\n",
        "      log_prior_prob = torch.sum(self.prior.log_prob(z), dim=(1, 2, 3))\n",
        "      return log_prior_prob + log_det_J\n",
        "\n",
        "  def loss(self, batch):\n",
        "      return torch.mean(-self.log_prob(batch)) / np.log(2) / 2\n",
        "  \n",
        "  def sample(self, size):\n",
        "        #z = self.prior.sample((size, 3, 32, 32))\n",
        "        z = self.prior.sample((size, 48, 8, 8))\n",
        "        return self.reverse(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NScng3Wns5Du",
        "colab_type": "code",
        "outputId": "14e3ae79-895b-4774-c919-64794c6d2b2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "#import torch_xla_py.xla_model as xm\n",
        "device = 'cuda'\n",
        "from tqdm import tqdm\n",
        "\n",
        "epochs = 8\n",
        "\n",
        "train_iter = torch.utils.data.DataLoader(data['train'], batch_size=64, shuffle=True)\n",
        "val_iter = torch.utils.data.DataLoader(data['test'], batch_size=64, shuffle=True)\n",
        "\n",
        "losses = []\n",
        "val_losses = []\n",
        "\n",
        "model = CelebA(3).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    tmp_losses = []\n",
        "    \n",
        "    for batch in tqdm(train_iter):\n",
        "        loss = model.loss(batch.to(device))\n",
        "        loss.backward()    \n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        losses.append(loss.data.cpu().numpy())\n",
        "        tmp_losses.append(loss.data.cpu().numpy())\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "        tmp_val_losses = []\n",
        "        for batch in val_iter:\n",
        "            val_loss = model.loss(batch.to(device))\n",
        "            val_losses.append(val_loss.data.cpu().numpy())\n",
        "            tmp_val_losses.append(val_loss.data.cpu().numpy())\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        print('Epoch {}: loss {} val_loss {} '.format(epoch,np.mean(tmp_losses),np.mean(tmp_val_losses)))\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(losses, label = \"train_loss\")\n",
        "plt.plot(np.arange(0, len(losses),int(len(losses)/len(val_losses))),val_losses, label = \"val_loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-296-090e485af8f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCelebA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 15.90 GiB total capacity; 14.81 GiB already allocated; 1.88 MiB free; 398.91 MiB cached)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHsM1jTVxVq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}